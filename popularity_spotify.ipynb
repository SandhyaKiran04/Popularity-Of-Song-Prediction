{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cubic-associate",
   "metadata": {},
   "source": [
    "## Research Question \n",
    "\n",
    "### Determine how well the features explain the popularity of songs on Spotify and Predict the popularity of a song based on these features(Using Spotify Data Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for colab\n",
    "!wget https://raw.githubusercontent.com/SandhyaKiran04/Popularity-Of-Song-Prediction/main/data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "diverse-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   sklearn.compose            import *\n",
    "from   sklearn.ensemble           import *\n",
    "from   sklearn.impute             import *\n",
    "from   sklearn.linear_model       import *\n",
    "from   sklearn.metrics            import * \n",
    "from   sklearn.pipeline           import Pipeline\n",
    "from   sklearn.model_selection    import RandomizedSearchCV\n",
    "from   sklearn.preprocessing      import *\n",
    "from   sklearn.model_selection    import train_test_split\n",
    "from   sklearn.base               import TransformerMixin,BaseEstimator\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-tissue",
   "metadata": {},
   "source": [
    "## Load Data and Examine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "committed-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sharp-beast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>artists</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>explicit</th>\n",
       "      <th>id</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>release_date</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.991000</td>\n",
       "      <td>['Mamie Smith']</td>\n",
       "      <td>0.598</td>\n",
       "      <td>168333</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0</td>\n",
       "      <td>0cS0A1fUEUd1EW3FcF8AEI</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3790</td>\n",
       "      <td>-12.628</td>\n",
       "      <td>0</td>\n",
       "      <td>Keep A Song In Your Soul</td>\n",
       "      <td>12</td>\n",
       "      <td>1920</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>149.976</td>\n",
       "      <td>0.6340</td>\n",
       "      <td>1920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.643000</td>\n",
       "      <td>[\"Screamin' Jay Hawkins\"]</td>\n",
       "      <td>0.852</td>\n",
       "      <td>150200</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0</td>\n",
       "      <td>0hbkKFIJm7Z05H8Zl9w30f</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0809</td>\n",
       "      <td>-7.261</td>\n",
       "      <td>0</td>\n",
       "      <td>I Put A Spell On You</td>\n",
       "      <td>7</td>\n",
       "      <td>1920-01-05</td>\n",
       "      <td>0.0534</td>\n",
       "      <td>86.889</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>1920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.993000</td>\n",
       "      <td>['Mamie Smith']</td>\n",
       "      <td>0.647</td>\n",
       "      <td>163827</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0</td>\n",
       "      <td>11m7laMUgmOKqI3oYzuhne</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5190</td>\n",
       "      <td>-12.098</td>\n",
       "      <td>1</td>\n",
       "      <td>Golfing Papa</td>\n",
       "      <td>4</td>\n",
       "      <td>1920</td>\n",
       "      <td>0.1740</td>\n",
       "      <td>97.600</td>\n",
       "      <td>0.6890</td>\n",
       "      <td>1920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000173</td>\n",
       "      <td>['Oscar Velazquez']</td>\n",
       "      <td>0.730</td>\n",
       "      <td>422087</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0</td>\n",
       "      <td>19Lc5SfJJ5O1oaxY0fpwfh</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1280</td>\n",
       "      <td>-7.311</td>\n",
       "      <td>1</td>\n",
       "      <td>True House Music - Xavier Santos &amp; Carlos Gomi...</td>\n",
       "      <td>17</td>\n",
       "      <td>1920-01-01</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>127.997</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>1920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.295000</td>\n",
       "      <td>['Mixe']</td>\n",
       "      <td>0.704</td>\n",
       "      <td>165224</td>\n",
       "      <td>0.707</td>\n",
       "      <td>1</td>\n",
       "      <td>2hJjbsLCytGsnAHfdsLejp</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>10</td>\n",
       "      <td>0.4020</td>\n",
       "      <td>-6.036</td>\n",
       "      <td>0</td>\n",
       "      <td>Xuniverxe</td>\n",
       "      <td>2</td>\n",
       "      <td>1920-10-01</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>122.076</td>\n",
       "      <td>0.2990</td>\n",
       "      <td>1920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acousticness                    artists  danceability  duration_ms  energy  \\\n",
       "0      0.991000            ['Mamie Smith']         0.598       168333   0.224   \n",
       "1      0.643000  [\"Screamin' Jay Hawkins\"]         0.852       150200   0.517   \n",
       "2      0.993000            ['Mamie Smith']         0.647       163827   0.186   \n",
       "3      0.000173        ['Oscar Velazquez']         0.730       422087   0.798   \n",
       "4      0.295000                   ['Mixe']         0.704       165224   0.707   \n",
       "\n",
       "   explicit                      id  instrumentalness  key  liveness  \\\n",
       "0         0  0cS0A1fUEUd1EW3FcF8AEI          0.000522    5    0.3790   \n",
       "1         0  0hbkKFIJm7Z05H8Zl9w30f          0.026400    5    0.0809   \n",
       "2         0  11m7laMUgmOKqI3oYzuhne          0.000018    0    0.5190   \n",
       "3         0  19Lc5SfJJ5O1oaxY0fpwfh          0.801000    2    0.1280   \n",
       "4         1  2hJjbsLCytGsnAHfdsLejp          0.000246   10    0.4020   \n",
       "\n",
       "   loudness  mode                                               name  \\\n",
       "0   -12.628     0                           Keep A Song In Your Soul   \n",
       "1    -7.261     0                               I Put A Spell On You   \n",
       "2   -12.098     1                                       Golfing Papa   \n",
       "3    -7.311     1  True House Music - Xavier Santos & Carlos Gomi...   \n",
       "4    -6.036     0                                          Xuniverxe   \n",
       "\n",
       "   popularity release_date  speechiness    tempo  valence  year  \n",
       "0          12         1920       0.0936  149.976   0.6340  1920  \n",
       "1           7   1920-01-05       0.0534   86.889   0.9500  1920  \n",
       "2           4         1920       0.1740   97.600   0.6890  1920  \n",
       "3          17   1920-01-01       0.0425  127.997   0.0422  1920  \n",
       "4           2   1920-10-01       0.0768  122.076   0.2990  1920  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "respiratory-adrian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174389, 19)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "presidential-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['key','mode','explicit']\n",
    "con_columns = ['acousticness','danceability','duration_ms','energy',\n",
    "               'instrumentalness','liveness','loudness','speechiness','tempo','valence','year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = spotify.drop('popularity', axis=1)  # features are taken separately \n",
    "target = spotify['popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pretty-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-pressure",
   "metadata": {},
   "source": [
    "## Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ongoing-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Dummy estimator to fit different models\n",
    "class DummyEstimator(BaseEstimator):                \n",
    "    \"Pass through class, methods are present but do nothing.\"\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "international-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing for categorical features:\n",
    "\n",
    "# 1) replacing missing values to most_frequent\n",
    "# 2) one hot encoding for changing strings to vectors\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(missing_values=np.nan,         \n",
    "                                              strategy='most_frequent')),   \n",
    "                    ('ohe', OneHotEncoder())])                              \n",
    "\n",
    "# preprocessing for continuous features:\n",
    "\n",
    "# 1) imputing missing values to median rather than mean to deal with outliers better\n",
    "# 2) Standscaler to normalize the features\n",
    "con_pipe = Pipeline([('imputer', SimpleImputer(strategy='median')),     \n",
    "                    ('scaler', StandardScaler())])\n",
    "\n",
    "\n",
    "# combining both the pipes into single preprocessing pipeline and drop the rest of columns\n",
    "\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, cat_columns),\n",
    "                                  ('continuous', con_pipe, con_columns)],remainder='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sustained-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipeline with dummy estiamtor\n",
    "pipe = Pipeline([('preprocessing', preprocessing),\n",
    "                 ('clf', DummyEstimator())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-yellow",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters on three Algorithms - Ridge, ExtraTreesRegressor and RandomForestRegressor\n",
    "### For Ridge, Tuning 'alpha' because it determines regularisation strength, 'solver' to determine which method to use in the computational routines.\n",
    "### For ExtraTreesRegressor and RandomForestRegressor, we are tuning 'n_estimators','max_depth','min_samples_leaf' and 'max_features' to reduce overfitting of the data and also whether to bootstrap the data or not  if set to 'False' the whole data is used to build the tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "persistent-vertex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   13.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(max_depth=10, min_samples_leaf=7, n_estimators=40)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_space = [{'clf':[Ridge()],\n",
    "                 'clf__alpha' : [0.5,1,1.5,2],\n",
    "                 'clf__max_iter': [100,500,1000],\n",
    "                 'clf__solver': ['auto','svd','cholesky','lsqr','sparse_cg','sag','saga']},\n",
    "    \n",
    "                {'clf': [ExtraTreesRegressor()],\n",
    "                 'clf__n_estimators': [20,30,40],\n",
    "                 'clf__max_depth': [10,20,30],\n",
    "                 'clf__min_samples_leaf': [3,5,7],\n",
    "                 'clf__max_features' : ['auto','sqrt','log2'],\n",
    "                 'clf__bootstrap' : [True,False]},\n",
    "                \n",
    "                {'clf': [RandomForestRegressor()],\n",
    "                 'clf__n_estimators':  [30,50,100],\n",
    "                 'clf__max_features':['auto', 'sqrt'],\n",
    "                 'clf__max_depth': [10,20,30],\n",
    "                 'clf__min_samples_leaf':[3,5,7],\n",
    "                 'clf__bootstrap': [True, False]} \n",
    "                ]\n",
    "                 \n",
    "\n",
    "clf_algos_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                                    param_distributions=search_space,\n",
    "                                    scoring = 'r2',\n",
    "                                    n_iter=5,\n",
    "                                    cv=3, \n",
    "                                    n_jobs=-1,\n",
    "                                    verbose=1)\n",
    "\n",
    "\n",
    "best_model = clf_algos_rand.fit(X_train, y_train)    # fitting the training data\n",
    "\n",
    "\n",
    "best_model.best_estimator_.get_params()['clf']       # best hyperparameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "upset-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred   = best_model.predict(X_test)               # predicting the values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-christian",
   "metadata": {},
   "source": [
    "### RMSE calculates a risk metric corresponding to the square root of expected value of the squared (quadratic) error or loss\n",
    "### R-Squared, explained variance score represents the proportion of variance that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance.\n",
    "### MAE calculates a risk metric corresponding to the expected value of the absolute error loss \n",
    "### Since we are interested in knowing how well the features explain the popularity of the song, we can consider R-Squared value or expalained variance score as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "romantic-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "var_score = explained_variance_score(y_test,y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "logical-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse:  13.628856802209896\n",
      "r2:  0.6127029975695873\n",
      "var_score:  0.6127033367720085\n",
      "mae:  9.326975440883418\n"
     ]
    }
   ],
   "source": [
    "print(\"rmse: \", rmse)\n",
    "print(\"r2: \", r2)\n",
    "print(\"var_score: \", var_score)\n",
    "print(\"mae: \",mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-purple",
   "metadata": {},
   "source": [
    "## Final  Best  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acting-index",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(transformers=[('categorical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  ['key', 'mode', 'explicit']),\n",
       "                                                 ('continuous',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['acousticness',\n",
       "                                                   'danceability',\n",
       "                                                   'duration_ms', 'energy',\n",
       "                                                   'instrumentalness',\n",
       "                                                   'liveness', 'loudness',\n",
       "                                                   'speechiness', 'tempo',\n",
       "                                                   'valence', 'year'])])),\n",
       "                ('clf',\n",
       "                 ExtraTreesRegressor(max_depth=10, min_samples_leaf=7,\n",
       "                                     n_estimators=40))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-duration",
   "metadata": {},
   "source": [
    "## Conclusion and Further Steps\n",
    "\n",
    "### R_Squared value came to be around 0.61 and it implies that model expalins 61% of the fitted data and that gives an indication that the features in the spotify data could be used to interpret the popularity of the songs.This  worked because the data set has diverse and predictive features.This is useful in business setting when trying to understand which songs could become popular based on features such as tempo,danceability etc., and could also be used  further to recommend songs to users based on the common features in the songs they listen to.\n",
    "\n",
    "### Further steps include improving the model fit by using other exhaustive methods like grid search.Also, finding if there is multicollinearity between the features and dropping those features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
